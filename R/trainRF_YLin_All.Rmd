---
title: "Обучение моделей типа Random Forest"
author: "Alexey Shovkun"
date: "29 сентября 2015 г."
output: html_document
---


```{r init, echo=FALSE, message=FALSE}
#install.packages("rpart")
#install.packages("ROCR")
#require (data.table)
require(caret) #dummyVars, featurePlot
#require(AppliedPredictiveModeling) #transparentTheme
#require(Hmisc) #cut2
#require(mice)
require(parallel) #detectCores()
require(doSNOW)
require (ggplot2)
require(gridECxtra)
require(rattle) #fancyRpartPlot
require(reshape2) #melt
require(rpart)
require(ROCR) #myPlotROC()
require (plyr); require(dplyr) #чтобы загружались именно в таком порядке

eval(parse('common.R',encoding = "UTF-8"))

nCores <- detectCores() 
#nCores <- 4
cl<-makeCluster(nCores - 1) # оставляем одно ядро на инфраструктуру и параллельные процессы
registerDoSNOW(cl) # Register the cores.


#learningCurvePoints <- ifelse (nCores < 11, 10, nCores-1) # Кол-во точек (моделей) для построения кривой обучения
learningCurvePoints <- 8
dfResults <- NULL
#Sys.getlocale() #en_US.UTF-8
#Sys.setlocale("LC_ALL", "ru_RU.UTF-8")
#sessionInfo()
trellis.par.set(caretTheme())


# загружаем списки параметров importantfeaturesN
load ("../data/features.rds")
```



Выделяем обучающую и тестовую выборки. Проверку качества модели в процессе подбора её параметров будем делать с использованием метода перекрестной проверки (cross validation) на обучающей выборке. Тестовая выборка будет использована **только** для оценки качества результирующей модели.
Если использовать всю обучающую выборку для выбора модели, то уходит очень много времени на рассчеты. Поэтому *выбор* модели будем делать на 20% от всех обучающих данных. Если при оценке какой-то модели окажется, что ее можно улучшить, увеличив размер обучающей выборки, то у нас будет такая возможность!

```{r makeSets, echo=FALSE, warning=FALSE, message = FALSE}
set.seed(20150415)
#dfTrainFull <- readRDS("../data/train_ConstMedianRange.rds")
#dfTrainFull <- readRDS("../data/train_ConstMedianYeoJohnsonRange.rds")
#dfTrainFull <- readRDS("../data/train_ConstMedianSelectedSure.rds")
#dfTrainFull <- readRDS("../data/train_ValueMedianRange.rds")
#dfTrainFull <- readRDS("../data/train_ValueMedianYeoJohnsonRange.rds")
#dfTrainFull <- readRDS("../data/train_ValueMedianSelectedSure.rds")
dfTrainAll <- readRDS("../data/train_ConstSubstMedianRange.rds") # Длинные категорийные заменены на ниболее вероятный класс
#dfTrainFull <- readRDS("../data/train_ConstSubstMedianYeoJohnsonRange.rds") # 
#dfTrainFull <- readRDS("../data/train_ConstSubstMedianSelectedSure.rds") # 
#class(dfTrainFull[,1]) #factor
#str(dfTrainFull)

## сокращаем объем исследуемых данных.
inSelection <- createDataPartition(dfTrainAll$y, p = .3, list = FALSE, times = 1)
dfTrain <- dfTrainAll[inSelection,]
inTrain <- createDataPartition(dfTrain$y, p = .75, list = FALSE, times = 1)
dfTest <- dfTrain[-inTrain,]
dfTrain <- dfTrain[inTrain,]
#summary(dfTrain)

# полный объем данных
inTrainFull <- createDataPartition(dfTrainAll$y, p = .75, list = FALSE, times = 1)
dfTestFull <- dfTrainAll[-inTrainFull,]
dfTrainFull <- dfTrainAll[inTrainFull,]
```

Размеры выборок: 
    
- Сокращенная обучающая: `r nrow(dfTrain)` экземпляров.

- проверочная: отсутствует, т.к. применяем кросс-валидацию.

- Сокращенная тестовая: `r nrow(dfTest)` экземпляров.

- Полная обучающая: `r nrow(dfTrainFull)` экземпляров.

- Полная тестовая: `r nrow(dfTestFull)` экземпляров.

### Предсказание редких категорий

Все построенные до сих пор модели плохо предсказывают возрастные группы 0 и 1. Вероятно, это связано с тем, что в обучающей выборке мало примеров абонентов  из указанных возрастных групп. Проведем теоретически сомнительный эксперимент: "размножим" данные примеров абонентов редких групп таким образом, чтобы количество примеров для абонентов каждой возрастной грпуппы было примерно одинаковым.

```{r predictRareAgeGroup, eval = FALSE}
dfTrainExt<-dfTrainFull
tab <- table(dfTrainFull$y)
tab

#str(table(dfTrainFull$y))
ratio <- floor(max(tab)/tab)
ratio
#class<-0
#ratio[class+1]
for (class in 0:6){
    dfSlice <- dfTrainFull[dfTrainFull$y == class,] # данные для категории (возрастная группа) class
    if(ratio[class+1] == 1) next
    for (i in 1:(ratio[class+1]-1)){ # повторяем данные
        dfTrainExt <- rbind(dfTrainExt, dfSlice)
    }    
}
table (dfTrainExt$y)


```

Бесполезный эксперимент.

Построим отдельную модель, которая будет предсказывать только возрастную группу "0/не 0". Случайный лес делает это плохо.

```{r trainRF0, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE}
dfTrainFull0 <- dfTrainFull
dfTrainFull0$y0 <- as.factor(dfTrainFull0$y==0)
#View(dfTrainFull0)
colFeaturesY0 <- setdiff(colnames(dfTrainFull0), c("y"))

nTree <- 20 #500

system.time({
    set.seed(1234)
    modSVM0 <- train (y0 ~ ., method="svmRadial", 
                    #data = dfTrainFull [importantFeatures1], #
                    #data = dfTrain [importantFeatures2], #
                    data = dfTrainFull0[colFeaturesY0], # 
                    metric = "ROC",
                    trControl = trainControl(method = "cv", number=10, repeats=5, 
                                             summaryFunction=twoClassSummary, classProbs=TRUE),
                    #количество деревьев в лесу.
                    #странно, что при увеличении параметра точность иногда ухудшается, а иногда улучшается
                    #ntree= nTree, 
#                     tuneGrid = expand.grid(mtry=c(
#                         #ceiling(sqrt(nPredictors)/5),
#                         #ceiling(sqrt(nPredictors)/4),
# #                         ceiling(sqrt(nPredictors)/3),
# #                         ceiling(sqrt(nPredictors)/2),
# #                         ceiling(sqrt(nPredictors)),
# #                         ceiling(sqrt(nPredictors))*2
# #                         , ceiling(nPredictors/3)
# #                         , ceiling(nPredictors/2)
# #                         , ceiling(nPredictors*2/3)
# #                         , nPredictors
#                         9, 11, 15, 19,  41 
#                         ))
                    tuneLength=3
    )
}) # 
modSVM0
ggplot(modRF0) # пказывает, что достаточно 50 деревьев
#modRF$finalModel
plot(modRF0$finalModel)
vi <- varImp(modRF0)
vi <- vi$importance 
vi$varName <-  modRF0$coefnames

viTop <- vi %>% arrange(desc(Overall)) %>% 
    top_n(20, Overall) 
viTop


acc <- modRF$results$Accuracy[as.numeric(rownames(modRF$bestTune)[1])]

paramRF <- modRF$bestTune$mtry
```



### Регрессия возраста

Представим возраст целым числом и попробуем построить регрессиию.
```{r addAgeFeature, echo=FALSE, eval=FALSE}
dfTrainAll$yLin <- as.numeric(dfTrainAll$y)
dfTrainFull$yLin <- as.numeric(dfTrainFull$y)
dfTrain$yLin <- as.numeric(dfTrain$y)
#View (dfTrainFull)
colFeaturesYLin <- setdiff(colnames(dfTrainFull), "y") # убираем y из набора фич, чтобы не мешался
colFeaturesYLin3 <- c("yLin",setdiff(importantFeatures3, "y")) # убираем y из набора фич, чтобы не мешался



```

Построим линейную модель, предсказывающую непрерывный возраст.
```{r predictAgeLinear, echo=FALSE, eval=TRUE}
system.time({
    set.seed(1234)
    modGLMBoost <- train (yLin ~ ., method="glmboost", 
                    data = dfTrainFull[colFeaturesYLin3], # 
                    trControl = trainControl(method = "cv", number=10, repeats=5)
                    #количество деревьев в лесу.
                    #странно, что при увеличении параметра точность иногда ухудшается, а иногда улучшается
                    #,ntree= nTree 
                    ,tuneGrid = expand.grid(
                        prune="no",
                        mstop=c(50, 100, 200, 300,500)
                        )
                    # ,tuneLength=5
    )
}) # 
modGLMBoost 
ggplot(modGLMBoost )


#varImp(modGLMBoost )

predicted  <- predict (modGLMBoost, newdata = dfTestFull)
qplot (x=predicted, y = dfTestFull$y)

# predicted  <- predict (modGLMBoost, newdata = dfTrainExt)
# qplot (x=predicted, y = dfTrainExt$y)

predicted  <- predict (modGLMBoost, newdata = dfTrainFull)
qplot (x=predicted, y = dfTrainFull$y)

```

Построим случайный лес модель, предсказывающий непрерывный возраст. Результат - не лучше линейной модели, но считается долго.
```{r predictAgeLinearRF, echo=FALSE, eval=FALSE}
nTree <- 50
system.time({
    set.seed(1234)
    modRFLin <- train (yLin ~ ., method="rf", 
                    data = dfTrainFull[colFeaturesYLin3], # 
                    trControl = trainControl(method = "cv", number=10, repeats=5)
                    #количество деревьев в лесу.
                    #странно, что при увеличении параметра точность иногда ухудшается, а иногда улучшается
                    #,ntree= nTree 
#                     ,tuneGrid = expand.grid(
#                         prune="no",
#                         mstop=c(50, 100, 200, 300,500, 1000)
#                         )
                     ,tuneLength=3
    )
}) # 13 час
modRFLin 
ggplot(modRFLin )


predicted  <- predict (modRFLin, newdata = dfTestFull)
qplot (x=predicted, y = dfTestFull$y)

# predicted  <- predict (modGLMBoost, newdata = dfTrainExt)
# qplot (x=predicted, y = dfTrainExt$y)

predicted  <- predict (modRFLin, newdata = dfTrainFull)
qplot (x=predicted, y = dfTrainFull$y)

```

Обновим тренировочные и тестовые датафреймы *предсказанными* линейными значениями возраста.
```{r updateDataAgeLinear, echo=FALSE}
dfTrainFull$yLin <- predict (modGLMBoost, newdata = dfTrainFull)
dfTrain$yLin <- predict (modGLMBoost, newdata = dfTrain)
dfTestFull$yLin <- predict (modGLMBoost, newdata = dfTestFull)
dfTest$yLin <- predict (modGLMBoost, newdata = dfTest)
dfTrainAll$yLin <- predict (modGLMBoost, newdata = dfTrainAll)

# dfTrainFull$yLin <- predict (modRFLin, newdata = dfTrainFull)
# dfTrain$yLin <- predict (modRFLin, newdata = dfTrain)
# dfTestFull$yLin <- predict (modRFLin, newdata = dfTestFull)
# dfTest$yLin <- predict (modRFLin, newdata = dfTest)

```

Обе рассмотренные модели (линейная и случайный лес) не очень хорошо предсказывают возраст. Обе склонны "завышать" возраст для возрастных групп 0 и 1. Построенные на основе обогащенных предсказаниями этих моделей модели Случйный лес и RSNNS дают точность ~69%.

### Случайный лес (Random Forest, rf)

 RF.

```
 DataType   Size   Feat nTree Time  mtry   Acc
 CSMNR      37502, все, 100    17m   81      75.26%
 CSMNR+Lin  37502, все, 100      m   39      75.19%
 CSMNR+Lin  50000, все, 100      m   39      75.19%


```

```{r trainRandomForest, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE}
#View (dfTrain[importantFeatures])
#str(dfTrain[importantFeatures])
#importantFeatures1 - все
#importantFeatures2 - кроме длинных категорийных
nPredictors = length(importantFeatures1) - 1 # из-за y
#nPredictors = length(allFeatures)#ncol(dfTrain)-1

nTree <- 100 #500
system.time({
    set.seed(1234)
    modRF <- train (y ~ ., method="rf", 
                    #data = dfTrainFull [importantFeatures1], #
                    data = dfTrainAll, #
                    #data = dfTrainFull, # 
                    trControl = trainControl(method = "cv", number=10, repeats=5),
                    #количество деревьев в лесу.
                    #странно, что при увеличении параметра точность иногда ухудшается, а иногда улучшается
                    ntree= nTree, 
                    tuneGrid = expand.grid(mtry=c(
                        #ceiling(sqrt(nPredictors)/5),
                        #ceiling(sqrt(nPredictors)/4),
#                         ceiling(sqrt(nPredictors)/3),
#                         ceiling(sqrt(nPredictors)/2),
#                         ceiling(sqrt(nPredictors)),
#                         ceiling(sqrt(nPredictors))*2
#                         , ceiling(nPredictors/3)
#                         , ceiling(nPredictors/2)
#                         , ceiling(nPredictors*2/3)
#                         , nPredictors
                        9, 21, 35, 37, 39, 41, 43, 80
                        ))
#                    tuneLength=3
    )
}) # 
modRF
ggplot(modRF) # пказывает, что достаточно 50 деревьев
#modRF$finalModel
plot(modRF$finalModel)
vi <- varImp(modRF)
vi <- vi$importance 
vi$varName <-  modRF$coefnames

viTop <- vi %>% arrange(desc(Overall)) %>% 
    top_n(20, Overall) 
viTop


acc <- modRF$results$Accuracy[as.numeric(rownames(modRF$bestTune)[1])]

paramRF <- modRF$bestTune$mtry
```



?Попытка обучить случайный лес на сокращенном наборе наиболее значимых параметров дает менее точный результат.


Проведем анализ сдвига/разброса на основе Случайного леса c параметром mtry =`r paramRF`.
```{r biasAndVarianceRF, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE}
# на сокращенных данных
myPlotLearningCurve (dfTrain, dfTest, method = "rf", targetVariable = "y", 
                     featuresList = importantFeatures1,
                     learningCurvePoints = learningCurvePoints,
                     tuneGrid = data.frame(
                                            mtry=modRF$bestTune$mtry
                                            ),
                     title = "Random Forest",
                     cvRepeats = 3,
                     ntree = nTree)
#69 min на полной (37к)

dfResults  <- rbind(dfResults,
                    data.frame (model="Random Forest", 
                          accuracy = acc, variance="Очень высокий"))

```

При nTree=50 график показывает, что обучающая ошибка постоянно равна 0, а тестовая ~ 25%. Это означает, что модель переобучена (имеет высокий разброс) и сдвинута (имеет высокий сдвиг). При nTree = 100 график показывает то же самое.

Чтобы побороть проблему переобучения, можно применть следующее:

 - регуляризация (см. далее),
 - уменьшить количество деревьев,
 - попробовать на полной обучающей выборке,
 - метод bagging,
 - больше данных,
 - сокращение (отбор) параметров.

Поскольку данная модель показывает адекватные результаты, выделим параметры абонента, которые для нее оказались важными. Будем использовать этот набор параметров в случаях, когда другие модели будут показывать высокий разброс.





### ?Продвинутое (boosted) Дерево решений (C5.0)

Предыдущий метод использовал набор одноуровневых деревьев для коллективного голосования, сейчас мы будем использовать сборку из многоуровневых деревьев. 

```{r trainC50, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=TRUE}
#View (dfTrain)
# trials	- an integer specifying the number of boosting iterations. 
#  A value of one indicates that a single model is used.
system.time({
    set.seed(12345)
    modC50 <- train (y ~ ., method="C5.0", 
                    #data = dfTrain [importantFeatures], #
                    #data = dfTrain [importantFeatures1], #73.67
                    data = dfTrain [importantFeatures3], #74.0
                    #preProcess= "spatialSign", #"pca"
                    trControl = trainControl(method = "cv", number=10, repeats=10)
                    ,tuneGrid = expand.grid(
                        winnow = FALSE,
                        model = c("rules"), #, "tree"
                        trials =  c(20, 40, 60, 80, 100)
                    )
    #                ,tuneLength=3
                    )
})
# 11255, IF1, trials = 20, model=rules, acc=73.7%
# 11255, IF1, 6min, trials = 60 , model=rules, acc=73.67%
# 11255, IF3, 7 min, trials =  60, model=rules, acc= 74.00%

#modC50
modC50$finalModel
#summary(modC50$finalModel)
#varImp (modC50)
ggplot(modC50)
#plot(modC50$finalModel)
#text (modC50$finalModel)

acc <- modC50$results[(modC50$results$model == modC50$bestTune$model) &
                      (modC50$results$winnow == modC50$bestTune$winnow) &
                      (modC50$results$trials == modC50$bestTune$trials), "Accuracy"]    
acc


```


Проведем анализ сдвига/разброса для Продвинутого дерева решений (C5.0):

 - model = **`r modC50$bestTune$model`**,  
 - winnow = **`r modC50$bestTune$winnow`**,  
 - trials = **`r modC50$bestTune$trials`**.

```{r biasAndVarianceС50, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=T}
# на сокращенных выборках
myPlotLearningCurve (dfTrain, dfTest, method = "C5.0", targetVariable = "y", 
                     featuresList = importantFeatures3,
                     learningCurvePoints = learningCurvePoints,
                     tuneGrid = data.frame(
                                           model = modC50$bestTune$model,
                                           winnow = modC50$bestTune$winnow,
                                           trials = modC50$bestTune$trials
                                            ),
                     title = "Продвинутое дерево решений (C5.0)",
                     cvRepeats = 5)
# при сокращенном обучающем наборе - средний (~15% vs 25%)

dfResults  <- rbind(dfResults,
                    data.frame (model="Продвинутое дерево решений (C5.0)", accuracy=acc,
                                variance="Высокий"))
```

Модель переобучена.

### ?Bagged CART

Поскольку Случайный лес переобучен, применим к нему технику bagging. Модель treebag не имеет параметров настройки.

```{r trainTreeBag, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=TRUE}
#View (dfTrain)
system.time({
    set.seed(12345)
    modTreeBag <- train (y ~ ., method="treebag", 
                    #data = dfTrain [importantFeatures1], #73.49
                    #data = dfTrain [importantFeatures3], #73.78
                    #data = dfTrain [importantFeatures4], #73.6
                    data = dfTrainFull [importantFeatures3], #74.11
                    trControl = trainControl(method = "cv", number=10, repeats=10)
#                     ,tuneGrid = expand.grid(
#                         winnow = FALSE,
#                         model = c("rules"), #, "tree"
#                         trials =  c(20, 40, 60, 80, 100)
#                     )
    #                ,tuneLength=3
                    )
})
# 11255, IF1, 2 min,  acc=73.49%
# 11255, IF3, 1 min, acc= 73.78%
# 11255, IF4, 20 sec, acc= 73.6%
# 37502, IF3, 4 min, acc= 74.11%

modTreeBag
#modTreeBag$finalModel
#summary(modTreeBag$finalModel)
#varImp (modTreeBag)
#ggplot(modTreeBag)
#plot(modTreeBag$finalModel)


acc <- modTreeBag$results[1, "Accuracy"]    
acc

```

```{r biasAndVarianceTreeBag, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=TRUE}
# на полных выборках
myPlotLearningCurve (dfTrainFull, dfTestFull, method = "treebag", targetVariable = "y", 
                     featuresList = importantFeatures3,
                     learningCurvePoints = learningCurvePoints,
                     tuneGrid = NULL,
                     title = "Bagged CART (treebag)",
                     cvRepeats = 3)
# при сокращенном обучающем наборе - средний (~15% vs 25%)

dfResults  <- rbind(dfResults,
                    data.frame (model="Bagged CART (treebag)", accuracy=acc,
                                variance="Очень высокий"))
```   

### ?Bagged AdaBoost (AdaBag)

?Попробуем еще один вариант баггинга.

```{r trainAdaBag, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=FALSE}
#View (dfTrain)
system.time({
    set.seed(20150527)
    modGBM <- train (y ~ ., method="AdaBag", 
                    data = dfTrainFull [importantFeatures3], #
                    #data = dfTrain [importantFeatures4], #
                    #data = dfTrain [allFeatures], #
                    #preProcess="pca",
                    trControl = trainControl(method = "cv", number=10, repeats=10)                        
                    #,n.minobsinnode = minObsInNode # minimum total weight needed in each node
                    ,verbose=FALSE # не выводить дели рассчета
#                     ,tuneGrid = expand.grid(
#                           )
                      ,tuneLength=3
)})
# Error in train.default(x, y, weights = w, ...) : 
#  final tuning parameters could not be determined
 
# 11255, IF3,  min, acc= 
# 37502, IF3, 


modAdaBag
modAdaBag$finalModel
#summary(modGBM$finalModel)
#gbm.perf (modGBM$finalModel)
#pretty.gbm.tree(modGBM$finalModel)
#varImp(modGBM)
ggplot(modAdaBag)

acc <- modGBM$results[(modGBM$results$n.trees == modGBM$bestTune$n.trees) & 
            (modGBM$results$interaction.depth == modGBM$bestTune$interaction.depth) &
            (modGBM$results$shrinkage == modGBM$bestTune$shrinkage), "Accuracy"]    
```

?Не получилось запустить эту модели из-за ошибки.

### ?Stochastic Gradient Boosting, ~AdaBoost (gbm)

An implementation of extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine. Includes regression methods for least squares, absolute loss, t-distribution loss, quantile regression, logistic, multinomial logistic, Poisson, Cox proportional hazards partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and Learning to Rank measures (LambdaMart).
http://en.wikipedia.org/wiki/Gradient_boosting

Параметры модели: 

 - **n.minobsinnode** - minimum total weight needed in each node.It is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances.
 - **n.trees** - количество деревьев, которые будут построены.
 - **interaction.depth** - the maximum depth of variable interactions. 1 implies an additive model, 2 implies a model with up to 2-way interactions, etc.

```{r trainGBM, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=FALSE}
#View (dfTrain)
#http://en.wikipedia.org/wiki/Gradient_boosting
#minObsInNode <- 11
system.time({
    set.seed(20150527)
    modGBM <- train (y ~ ., method="gbm", 
                    data = dfTrain [importantFeatures3], #74.79
                    #data = dfTrain [importantFeatures4], #
                    #data = dfTrain [allFeatures], #
                    #preProcess="pca",
                    trControl = trainControl(method = "cv", number=10, repeats=10)                        
                    #,n.minobsinnode = minObsInNode # minimum total weight needed in each node
                    ,verbose=FALSE # не выводить дели рассчета
                    ,tuneGrid = expand.grid(
                        #the total number of trees to fit. This is equivalent to the number 
                        #of iterations and the number of basis functions in the additive expansion
                        n.trees = c(80, 100, 200, 300), #1e3, 1e4 #кол-во деревьев на 500 уже не хватает 30ГБ ОЗУ
                        #the maximum depth of variable interactions. 1 implies an additive model, 
                        #2 implies a model with up to 2-way interactions, etc.
                        interaction.depth = c(2,3,4), 
                        #a shrinkage parameter applied to each tree in the expansion. 
                        #Also known as the learning rate or step-size reduction
                        shrinkage= c(0.1) #learning rate #0.01, 0.1,1, 1.1
                          )
#                      ,tuneLength=3
)})
# 11255, IF3, 1 min, acc= 74.79%, , n.trees=100, i.depth = 2, shrinkage = 0.1
# 11255, IF3, 4 min, acc= 74.75%, n.trees=100, i.depth = 4, shrinkage = 0.1
# 37502, IF3, 


modGBM
modGBM$finalModel
#summary(modGBM$finalModel)
#gbm.perf (modGBM$finalModel)
#pretty.gbm.tree(modGBM$finalModel)
#varImp(modGBM)
ggplot(modGBM)

acc <- modGBM$results[(modGBM$results$n.trees == modGBM$bestTune$n.trees) & 
            (modGBM$results$interaction.depth == modGBM$bestTune$interaction.depth) &
            (modGBM$results$shrinkage == modGBM$bestTune$shrinkage), "Accuracy"]    
```

Ничего интересного в результатах. Далее не рассматриваем.

?Проведем анализ сдвига/разброса на основе Stochastic Gradient Boosting c параметрами:
 - n.trees = **r modGBM$bestTune$n.trees**, 
 - interaction.depth = **r modGBM$bestTune$interaction.depth**, 
 - shrinkage = **r modGBM$bestTune$shrinkage**.

```{r biasAndVarianceGBM, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
set.seed(1234)
#library(caret)
res <- data.frame()
res <- foreach  (m = ceiling(seq (nrow(dfTrain)*0.7, nrow(dfTrain), length.out=10)), 
                 .combine=rbind) %dopar% {    
                     #m=51
    rows <- sample (1:nrow(dfTrain),m)    
    mod <- caret::train (class ~ ., method="gbm", 
                  data = dfTrain[importantFeatures][rows, ],
                  trControl = caret::trainControl(method = "cv", number=10, repeats=5),
                  n.minobsinnode = minObsInNode ,
                  tuneGrid = data.frame(
                        n.trees = modGBM$bestTune$n.trees,
                        interaction.depth = modGBM$bestTune$interaction.depth, 
                        shrinkage = modGBM$bestTune$shrinkage)
                  )    
    
    #as.numeric(predict (mod$finalModel, newData=dfTrain[importantFeatures][rows, ])[,2]>0.5)  
    predictionsTrain <- predict (mod, newdata = dfTrain[importantFeatures][rows, ])
    accTrain <- caret::confusionMatrix(predictionsTrain, dfTrain$class[rows], positive="1")$overall[1]
    predictions <- predict (mod, newdata = dfTest)
    accTest <- caret::confusionMatrix(predictions,dfTest$class, positive="1")$overall[1]
    #res <- rbind(res, data.frame(m=m, errorTrain = 1-accTrain, errorTest = 1-accTest))
    rbind(res, data.frame(m=m, errorTrain = 1-accTrain, errorTest = 1-accTest))
}

# на сокращенных выборках
myPlotLearningCurve (dfTrain, dfTest, method = "gbm", targetVariable = "y", 
                     featuresList = importantFeatures3,
                     learningCurvePoints = learningCurvePoints,
                     tuneGrid = data.frame(
                                n.trees = modGBM$bestTune$n.trees,
                                interaction.depth = modGBM$bestTune$interaction.depth, 
                                shrinkage = modGBM$bestTune$shrinkage)
                                ),
                     title = "Stochastic Gradient Boosting (gbm)",
                     cvRepeats = 3)


dfResults  <- rbind(dfResults,
                    data.frame (model="Stochastic Gradient Boosting (gbm)", 
                          accuracy=acc, variance = "Высокий"))

```




### Анализ качества обученных моделей

```{r results, echo=FALSE, warning=FALSE, message = FALSE}
dfResults
```



### Проверка лучшей модели на тестовой выборке 
 
В качестве победившей по качеству предсказания на обучающих данных выбираем модель "Случайный лес (rf) ". Оценим качество ее предсказания на тестовой выборке, которую модель не "видела" при обучении.
 
```{r checkAccuracy, echo=FALSE, warning=FALSE, message = FALSE}
# делаем свою функцию, чтобы исправить баг библиотеки caret 6.0-41
myPredict.train <- function (object, newdata = NULL, type = "raw", na.action = na.omit, 
    ...) 
{
    if (all(names(object) != "modelInfo")) {
        object <- update(object, param = NULL)
    }
    if (!is.null(object$modelInfo$library)) 
        for (i in object$modelInfo$library) do.call("require", 
            list(package = i))
    if (!(type %in% c("raw", "prob"))) 
        stop("type must be either \"raw\" or \"prob\"")
    if (type == "prob") {
        if (is.null(object$modelInfo$prob)) 
            stop("only classification models that produce probabilities are allowed")
    }
    if (!is.null(newdata)) {
        if (inherits(object, "train.formula")) {
            newdata <- as.data.frame(newdata)
            rn <- row.names(newdata)
            Terms <- delete.response(object$terms)
            m <- model.frame(Terms, newdata, na.action = na.action, 
                xlev = object$xlevels)
            if (!is.null(cl <- attr(Terms, "dataClasses"))) 
                .checkMFClasses(cl, m)
            keep <- match(row.names(m), rn)
            newdata <- model.matrix(Terms, m, contrasts = object$contrasts)
            xint <- match("(Intercept)", colnames(newdata), nomatch = 0)
            if (xint > 0) 
                newdata <- newdata[, -xint, drop = FALSE]
        }
    }
    else {
        if (!is.null(object$trainingData)) {
            newdata <- if (object$method == "pam") 
                object$finalModel$xData
            else object$trainingData
        }
        else stop("please specify data via newdata")
    }
    if (type == "prob") {
        out <- extractProb(list(object), unkX = newdata, unkOnly = TRUE, 
            ...)
        obsLevels <- levels(object)
        out <- out[, paste0("X", obsLevels), drop = FALSE]
    }
    else {
        out <- extractPrediction(list(object), unkX = newdata, 
            unkOnly = TRUE, ...)$pred
    }
    out
}

#modFinal <- modGLMBoost #
#modFinal <- modSVM
#modFinal <- modLB #70,64% (полное обучение/тест)
#modFinal <- modDT # 
modFinal <- modRF # 75,92% (полное обучение/тестб CSMNR)
#modFinal <- modGBM #
#modFinal <- modRSNNS #
#modFinal <- modNNET #
#modFinal <- modGLMBoost # 71% на importantFeatures2, 75% на allFeatures
#debugonce(predict)
#predict(modFinal, newdata=dfTest, type="prob")

# Для всех, кроме modLB:
predTest <- predict(modFinal, newdata=dfTestFull) 
# # для modLB:
# predTest <- myPredict.train(modFinal, newdata=dfTestFull, type = "prob") 
# predTest <- (apply(predTest,1, which.max) - 1)
# predTest <- as.factor(predTest)

cm <- caret::confusionMatrix(data = predTest, reference = dfTestFull$y,
                             positive= "1")
accFinal <- cm$overal[1]
cm
```

## Предсказние новых 

```{r predictNew, echo=FALSE, warning=FALSE, message = FALSE, eval=TRUE}

#dfFinalTest <- readRDS("../data/test_ConstMedianRange.rds")
dfFinalTest <- readRDS("../data/test_ConstSubstMedianRange.rds")

# предсказываем дополнительный параметр
dfFinalTest$yLin <- predict (modGLMBoost, newdata = dfFinalTest)

#table (dfTrainFull$y)
#apply(dfFinalTest, 2, function(x)sum(is.na(x)))

# для всех, кроме modLB (LogitBoost)
predicted <- predict (modFinal, newdata = dfFinalTest)
# # для LogitBoost
# # LogitBoost дает предсказание NA, если два класса получили одинаковые голоса.
# predicted <- myPredict.train (modLB, newdata = dfFinalTest, type = "prob") 
# predicted$y <- apply(predicted,1, which.max) - 1
# #View(predicted)
# predicted <- as.factor(predicted$y)



#03.10.15 - RF, сокр 30% = 74.91%
#05.10.15 - LogitBoost, полн 100% = 70.81%
#12.10.15 - RF на CSMNR, полн 100% = 75.73%
#12.10.15 - RF на CSMNR+yLin, полн 100% = 75.77%
#20.10.15 - RF на CSMNR+yLin, все данные тренировочные (50к) = 75.95%

dfPredicted <- data.frame (ID = dfFinalTest$ID, y = predicted,
                           stringsAsFactors = FALSE)
#View(dfPredicted)


write.table(dfPredicted, "../data/151009_sol_CostSubstMedianRange_yLin_All.csv", sep=",", quote=FALSE, row.names=FALSE)
```


Результаты публикации:

 - для модели Случайный лес (rf) точность оказалась **74.91%**. Показатель невысокий, однако это первая публикация и она показала, что применяемый для выбора модели подход для оценки точности предстказания работает хорошо - в реальности точность оказалась даже чуть лучше, чем ожидалось.  
 - для модели (LogitBoost) точность оказалась **70.81%**. Низкая точность обусловлена спефикой модели (NA в предсказанных классах).  - для модели случайный лес на данных CSMNR (NA в категориях заменены на константу, категории в длинных переменных (> 5 категорий) заменены на наиболее вероятный для них класс, NA в новых категориях тестового примера также заменены на константу, числовые переменные преобразованы линейно в диапазон [0,1] ) точность оказалась **75.73**.
- для модели случайный лес, аналогичной предыдущей, к которой был добавлен предсказанный непрерывный параметр возраста (yLin),  точность оказалась **75.77** .
 
 

```{r stopCluster, echo=FALSE, warning=FALSE, message = FALSE, cache=FALSE}
stopCluster(cl) # Explicitly free up cores again.

```
