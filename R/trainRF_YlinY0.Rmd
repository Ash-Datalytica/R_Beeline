---
title: "Обучение моделей типа Random Forest"
author: "Alexey Shovkun"
date: "29 сентября 2015 г."
output: html_document
---


```{r init, echo=FALSE, message=FALSE}
#install.packages("rpart")
#install.packages("ROCR")
#install.packages("ROSE")
#require (data.table)
require(caret) #dummyVars, featurePlot
#require(AppliedPredictiveModeling) #transparentTheme
#require(Hmisc) #cut2
#require(mice)
require(parallel) #detectCores()
require(doSNOW)
require (ggplot2)
require(gridECxtra)
require(rattle) #fancyRpartPlot
require(reshape2) #melt
require(rpart)
require(ROCR) #myPlotROC()
require (plyr); require(dplyr) #чтобы загружались именно в таком порядке
require (ROSE) # for oversampling

eval(parse('common.R',encoding = "UTF-8"))

#nCores <- detectCores() 
nCores <- 10 # чтобы уместиться в ОЗУ
cl<-makeCluster(nCores - 1) # оставляем одно ядро на инфраструктуру и параллельные процессы
registerDoSNOW(cl) # Register the cores.


#learningCurvePoints <- ifelse (nCores < 11, 10, nCores-1) # Кол-во точек (моделей) для построения кривой обучения
learningCurvePoints <- 8
dfResults <- NULL
#Sys.getlocale() #en_US.UTF-8
#Sys.setlocale("LC_ALL", "ru_RU.UTF-8")
#sessionInfo()
trellis.par.set(caretTheme())


# загружаем списки параметров importantfeaturesN
load ("../data/features.rds")
```



Выделяем обучающую и тестовую выборки. Проверку качества модели в процессе подбора её параметров будем делать с использованием метода перекрестной проверки (cross validation) на обучающей выборке. Тестовая выборка будет использована **только** для оценки качества результирующей модели.
Если использовать всю обучающую выборку для выбора модели, то уходит очень много времени на рассчеты. Поэтому *выбор* модели будем делать на 20% от всех обучающих данных. Если при оценке какой-то модели окажется, что ее можно улучшить, увеличив размер обучающей выборки, то у нас будет такая возможность!

```{r makeSets, echo=FALSE, warning=FALSE, message = FALSE}
set.seed(20150415)
#dfTrainFull <- readRDS("../data/train_ConstMedianRange.rds")
#dfTrainFull <- readRDS("../data/train_ConstMedianYeoJohnsonRange.rds")
#dfTrainFull <- readRDS("../data/train_ConstMedianSelectedSure.rds")
#dfTrainFull <- readRDS("../data/train_ValueMedianRange.rds")
#dfTrainFull <- readRDS("../data/train_ValueMedianYeoJohnsonRange.rds")
#dfTrainFull <- readRDS("../data/train_ValueMedianSelectedSure.rds")
dfTrainAll <- readRDS("../data/train_ConstSubstMedianRange.rds") # Длинные категорийные заменены на ниболее вероятный класс
#dfTrainFull <- readRDS("../data/train_ConstSubstMedianYeoJohnsonRange.rds") # 
#dfTrainFull <- readRDS("../data/train_ConstSubstMedianSelectedSure.rds") # 
#class(dfTrainFull[,1]) #factor
#str(dfTrainFull)

## сокращаем объем исследуемых данных.
inSelection <- createDataPartition(dfTrainAll$y, p = .3, list = FALSE, times = 1)
dfTrain <- dfTrainAll[inSelection,]
inTrain <- createDataPartition(dfTrain$y, p = .75, list = FALSE, times = 1)
dfTest <- dfTrain[-inTrain,]
dfTrain <- dfTrain[inTrain,]
#summary(dfTrain)

# полный объем данных
inTrainFull <- createDataPartition(dfTrainAll$y, p = .75, list = FALSE, times = 1)
dfTestFull <- dfTrainAll[-inTrainFull,]
dfTrainFull <- dfTrainAll[inTrainFull,]
```

Размеры выборок: 
    
- Сокращенная обучающая: `r nrow(dfTrain)` экземпляров.

- проверочная: отсутствует, т.к. применяем кросс-валидацию.

- Сокращенная тестовая: `r nrow(dfTest)` экземпляров.

- Полная обучающая: `r nrow(dfTrainFull)` экземпляров.

- Полная тестовая: `r nrow(dfTestFull)` экземпляров.


### Регрессия возраста

Представим возраст целым числом и попробуем построить регрессиию.
```{r addAgeFeature, echo=FALSE, eval=TRUE}
dfTrainAll$yLin <- as.numeric(dfTrainAll$y)
dfTrainFull$yLin <- as.numeric(dfTrainFull$y)
dfTrain$yLin <- as.numeric(dfTrain$y)
#View (dfTrainFull)
colFeaturesYLin <- setdiff(colnames(dfTrainFull), "y") # убираем y из набора фич, чтобы не мешался
colFeaturesYLin3 <- c("yLin",setdiff(importantFeatures3, "y")) # убираем y из набора фич, чтобы не мешался



```

Построим линейную модель, предсказывающую непрерывный возраст. Используем ВСЕ имеющиеся обучающие данные.
```{r predictAgeLinear, echo=FALSE, eval=TRUE, cache=TRUE}
system.time({
    set.seed(1234)
    modGLMBoost <- train (yLin ~ ., method="glmboost", 
                    data = dfTrainAll[colFeaturesYLin3], # для финального рассчета
                    #data = dfTrainFull[colFeaturesYLin3], # для отладки - чтобы смотреть на точность предсказания
                    trControl = trainControl(method = "cv", number=10, repeats=5)
                    #количество деревьев в лесу.
                    #странно, что при увеличении параметра точность иногда ухудшается, а иногда улучшается
                    #,ntree= nTree 
                    ,tuneGrid = expand.grid(
                        prune="no",
                        mstop=c(50, 100, 200, 300,500)
                        )
                    # ,tuneLength=5
    )
}) # 
modGLMBoost 
ggplot(modGLMBoost )


#varImp(modGLMBoost )

predicted  <- predict (modGLMBoost, newdata = dfTestFull)
qplot (x=predicted, y = dfTestFull$y)

# predicted  <- predict (modGLMBoost, newdata = dfTrainExt)
# qplot (x=predicted, y = dfTrainExt$y)

predicted  <- predict (modGLMBoost, newdata = dfTrainFull)
qplot (x=predicted, y = dfTrainFull$y)

```
Видим, что модель не очень сильная, особенно плохо работает на редких возрастных группах 0 и 1, но в целом  улавливает тренд.


Обновим тренировочные и тестовые датафреймы *предсказанными* линейными значениями возраста.

```{r updateDataAgeLinear, echo=FALSE}
dfTrainFull$yLin <- predict (modGLMBoost, newdata = dfTrainFull)
dfTrain$yLin <- predict (modGLMBoost, newdata = dfTrain)
dfTestFull$yLin <- predict (modGLMBoost, newdata = dfTestFull)
dfTest$yLin <- predict (modGLMBoost, newdata = dfTest)
dfTrainAll$yLin <- predict (modGLMBoost, newdata = dfTrainAll)

```

Обе рассмотренные модели (линейная и случайный лес) не очень хорошо предсказывают возраст. Обе склонны "завышать" возраст для возрастных групп 0 и 1. Построенные на основе обогащенных предсказаниями этих моделей модели Случйный лес и RSNNS дают точность ~69%.

### Предсказание редких категорий

?oversampling Все построенные до сих пор модели плохо предсказывают возрастные группы 0 и 1. Вероятно, это связано с тем, что в обучающей выборке мало примеров абонентов  из указанных возрастных групп. Проведем теоретически сомнительный эксперимент: "размножим" данные примеров абонентов редких групп таким образом, чтобы количество примеров для абонентов каждой возрастной грпуппы было примерно одинаковым.


Построим отдельную модель, которая будет предсказывать только возрастную группу "0/не 0". Случайный лес делает это плохо. Хорошую модель найти не удалось. Лучший результат дает модель С5.0 (усиленный случайный лес) на расширенном (oversampled) наобре тренировочных данных. Параметр Cost позволяет увеличить цену ошибок FN по сравнению с TP, т.е. сократить FN, возможно, увеличив TP. D нашем случае это интересно, но большие значения не подходят, т.к. мы используем уже сбалансированную по числу случаем для каждого класса обучающую выборку. Путем перебора разных значений параметров Cost и trials с целью получения минимального количества FN и максимального TP на *тестовой* выборке при FN На тренировочной = 0, определяем оптимальные значения Cost=3, trials = 5.

```{r dataModN, echo=FALSE, warning = FALSE}
dfTrainFull$y0 <- as.factor(ifelse(dfTrainFull$y==0, 1, 0))
dfTrainFull$y1 <- as.factor(ifelse(dfTrainFull$y==1, 1, 0))
dfTrainFull$y6 <- as.factor(ifelse(dfTrainFull$y==6, 1, 0))
#View(dfTrainFull0)

dfTrainAll$y0 <- as.factor(ifelse(dfTrainAll$y==0, 1, 0))

#dfTrain0 <- dfTrain
#dfTrain$y0 <- as.factor(ifelse(dfTrain$y==0, 1, 0))
#dfTrain$y1 <- as.factor(ifelse(dfTrain$y==1, 1, 0))

#dfTrainExt$y0 <- as.factor(ifelse(dfTrainExt$y == 0, 1, 0))

#исключаем yLin из иптиска параметров, чтобы не было "зацикливания" на предсказанных линейной моделью значеий
# таким образом по сути строим ансамбль моделей: Линейная, SVM и тп.
colFeaturesY0 <- c("y0",setdiff(colnames(dfTrainFull), c("y", "yLin","y1","y2","y3","y4","y5","y6")))
colFeaturesY1 <- c("y1",setdiff(colnames(dfTrainFull), c("y", "yLin", "y0","y1","y2","y3","y4","y5","y6")))
colFeaturesY1 <- c("y6",setdiff(colnames(dfTrainFull), c("y", "yLin", "y0","y1","y2","y3","y4","y5","y6"))) 
importantFeatures1Y0 <- c("y0",setdiff(importantFeatures1, c("y", "yLin","y1"))) 
table(dfTrainAll$y)

```


```{r trainMod0, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE}


########
# varImp(modGLMBoost)
#     Overall
# x30 100.000
# x8   70.116
# x56  49.949
# x35  38.077
# x59  29.851
# x23  23.535
# x43  22.155
# x44  20.151
# x58  20.078
# x60  19.990
# x32  18.655
# x39  15.333
# x29  12.335
# x42  11.345
# x41  11.328
# x31  10.599
# x55   9.187
# x40   8.682
# x13   8.272
# x36   6.182

#LogitBoost на dfTrainExt[colFeaturesY0]
#     Importance
# x57     100.00
# x56      97.23
# x37      95.32
# x30      91.68
# x53      90.57
# x23      87.85
# x38      85.80
# x24      84.80
# x54      83.35
# x25      83.09
# x26      80.61
# x32      79.22
# x36      76.69
# x33      75.62
# x59      74.86
# x40      74.25
# x13      73.05
# x61      72.62
# x28      72.49
# x39      71.20
######

# table(dfTrainFull$y0)
# dfTrainFullBalanced0 <- ovun.sample(y0 ~ ., data = dfTrainFull, method = "over", p=0.5)$data #oversampling
# table(dfTrainFullBalanced0$y0)

table(dfTrainAll$y0)
dfTrainAllBalanced0 <- ovun.sample(y0 ~ ., data = dfTrainAll, method = "over", p=0.5)$data #oversampling
table(dfTrainAllBalanced0$y0)

system.time({
    set.seed(1234)
    mod0 <- train (y0~x8+x23+x24+x30+x35+x37+x38+x39+x43+x44+x56+x57+x59, #y0 ~ .
                   #method="rf", #BalancedAccuracy = .50
                   ## Boosting methods
                    #method="LogitBoost", #BalancedAccuracy = .67
                    #method="glmboost", #final tuning parameters could not be determined
                    #method="ada", # final tuning parameters could not be determined
                    #method="AdaBoost.M1", # долго!!!
                   ## Cost-sensitive methods
                    method = "C5.0Cost",# меньше столбцов!, Balanced Accuracy = 0,52
                    #data = dfTrain0 [importantFeatures1Y0], #
                    #data = dfTrain0 [c(rows0,rowsNot0Sample), colFeaturesY0], #
                    #data = dfTrainFull0[colFeaturesY0], #
                    data = dfTrainAllBalanced0, 
                    #data = dfTrainFullBalanced0, #
                    trControl = trainControl(method = "cv", number=10, repeats=3),
                   ## LogitBoost
                    #tuneGrid = data.frame(nIter=c(31))
                   ##C5.0Cost
                    tuneGrid = expand.grid(trials=c(5),
                                           model="tree",
                                           winnow=FALSE,
                                           cost=c(3)
                       )
#                     tuneLength=3
    )
}) # 

mod0
#summary(mod0)
#ggplot(mod0)
varImp(mod0)

# caret::confusionMatrix(data = predict(mod0, newdata = dfTrainFullBalanced0), 
#                              reference = dfTrainFullBalanced0$y0,
#                              positive="1")


# predictedTest <- predict (mod0, newdata = dfTestFull)
# #table(predictedTest)
# caret::confusionMatrix(data = predictedTest, 
#                              reference = as.factor(ifelse(dfTestFull$y==0,1,0)),
#                              positive="1")
# 
# dfTrainFull$y0 <- predict(mod0, newdata=dfTrainFull)
# dfTestFull$y0 <- predictedTest
 dfTrainAll$y0 <- predict(mod0, newdata=dfTrainAll)

```
Среди результатов сначала видим распределение целевого класса (1 = "Возрастная группа = 0", 0 = "Возрастная группа <> 0") до расширения выборки, а потом - после расширения тренировочного набора данных. Расшиение выборки произошло за счет дублирования примеров из класса 1.

Первая матрица неточностей показывает, что модель предсказывает всех пользователей из возрастной группы 0 (class=1) (FN=0) и при этом имеет неплохую Balanced Accuracy=99.31% на обучающих данных. Вторая матрица неточностей построена для тестовых данных и показывает Balanced Accuracy =55.84%. В идеале хотелось бы иметь на тестовых данных также FN=0 и более высокую BA. 

Построим модель, предсказывающую возрастную группу 1. Лучше всего конечный результат при C=1, но все равно он хуже, чем без этой модели. Пока не будем ее использовать. Возможно, стоит попробовать другой тип можели для более сильной классификации, например, 2-class SVM.

```{r trainMod1, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=FALSE}


table(dfTrainFull$y1)
dfTrainFullBalanced1 <- ovun.sample(y1 ~ ., data = dfTrainFull, method = "over", p=0.5)$data #oversampling
table(dfTrainFullBalanced1$y1)

# inFeatureSelect <- createDataPartition(dfTrainFullBalanced1$y1, p = .1, list = FALSE, times = 1)
# system.time({
#     set.seed(1234)
#     modTmp <- train (y1 ~ .,
#                    ## Cost-sensitive methods
#                     method = "rf",# меньше столбцов!
#                     #data = dfTrainFull0[colFeaturesY0], #
#                     #data = dfTrainExt[importantFeatures1Y0], #
#                     data = dfTrainFullBalanced1[inFeatureSelect,colFeaturesY1], # 
#                     nTree=50,
#                     trControl = trainControl(method = "cv", number=10, repeats=3),
#                    ##C5.0Cost
#                     tuneGrid = expand.grid(                        
#                         ## RF
#                         mtry=c(9,11,15,21, 35, 37)
#                        )
# #                     tuneLength=3
#     )
# }) # 
# 
# #ggplot(modTmp)
# 
# vi <- varImp(modTmp)
# vi <- vi$importance 
# vi$varName <-  modTmp$coefnames
# viTop <- vi %>% arrange(desc(Overall)) %>% 
#     top_n(50, Overall) 
# viTop
##
#       Overall varName
# 1  100.000000      x8
# 2   99.673365     x55
# 3   57.915719     x30
# 4   34.787697     x23
# 5   25.939540     x29
# 6   25.513466     x37
# 7   22.598059     x24
# 8   21.156305     x13
# 9   19.988994     x56
# 10  19.753781     x53
# 11  19.039184     x59
# 12  18.973757     x54
# 13  18.728453     x25
# 14  18.388976    x172
# 15  17.393720     x57
# 16  16.581741     x27
# 17  15.889888     x28
# 18  15.499069     x58
# 19  14.949691     x61
# 20  14.631145     x43
# 21  14.396400    x173
# 22  14.118094     x40
# 23  13.945378     x31
# 24  13.672356     x33
# 25  13.411884     x32
# 26  13.351131     x41
# 27  12.963184     x26
# 28  11.999328     x42
# 29  10.982230    x181
# 30  10.830362    x114

#берем первые 25 переменных
system.time({
    set.seed(1234)
    mod1 <- train (y1~x8+x55+x30+x23+x29+x37+x24+x13+x56+x53+x59+x54+x25+x17+x57+x27+x28+x58+x61+x43+x40+x31+x33+x32, #y1 ~ .
                   ## Cost-sensitive methods
                    method = "C5.0Cost",# меньше столбцов!
                    #data = dfTrainFull0[colFeaturesY0], #
                    #data = dfTrainExt[importantFeatures1Y0], #
                    data = dfTrainFullBalanced1, # 
                    trControl = trainControl(method = "cv", number=10, repeats=3),
                   ##C5.0Cost
                    tuneGrid = expand.grid(trials=c(10), #50 - хуже
                                           model="tree",
                                           winnow=FALSE,
                                           cost=c(1)
                       )
#                     tuneLength=3
    )
}) # 

mod1 
#summary(mod1)
#ggplot(mod1)
#varImp(mod1)

caret::confusionMatrix(data = predict(mod1, newdata = dfTrainFullBalanced1), 
                             reference = dfTrainFullBalanced1$y1,
                             positive="1")


predictedTest <- predict (mod1, newdata = dfTestFull)
#table(predictedTest)
caret::confusionMatrix(data = predictedTest, 
                             reference = as.factor(ifelse(dfTestFull$y==1,1,0)),
                             positive="1")

dfTrainFull$y1 <- predict(mod1, newdata=dfTrainFull)
dfTestFull$y1 <- predictedTest
dfTrainAll$y1 <- predict(mod1, newdata=dfTrainAll)

```

Следующие по величине ошибки возрастные группы №6, №5 и №4. В следующих версиях сосредоточимся на них.

```{r trainMod6, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=FALSE}


table(dfTrainFull$y6)
dfTrainFullBalanced6 <- ovun.sample(y6 ~ ., data = dfTrainFull, method = "over", p=0.5)$data #oversampling
table(dfTrainFullBalanced6$y6)

system.time({
    set.seed(1234)
    mod6 <- train (y6~x8+x23+x24+x30+x35+x37+x38+x39+x43+x44+x56+x57+x59, #y0 ~ .
                   ## Cost-sensitive methods
                    method = "C5.0Cost",# меньше столбцов!
                    #data = dfTrainFull0[colFeaturesY0], #
                    #data = dfTrainExt[importantFeatures1Y0], #
                    data = dfTrainFullBalanced6, #
                    trControl = trainControl(method = "cv", number=10, repeats=3),
                   ##C5.0Cost
                    tuneGrid = expand.grid(trials=c(10), #50 - хуже
                                           model="tree",
                                           winnow=FALSE,
                                           cost=c(1)
                       )
#                     tuneLength=3
    )
}) # 

mod6
#summary(mod6)
#ggplot(mod6)
#varImp(mod6)

caret::confusionMatrix(data = predict(mod6, newdata = dfTrainFullBalanced6), 
                             reference = dfTrainFullBalanced6$y6,
                             positive="1")

predictedTest <- predict (mod6, newdata = dfTestFull)
#table(predictedTest)
caret::confusionMatrix(data = predictedTest, 
                             reference = as.factor(ifelse(dfTestFull$y==6,1,0)),
                             positive="1")

dfTrainFull$y6 <- predict(mod6, newdata=dfTrainFull)
dfTestFull$y6 <- predictedTest
dfTrainAll$y6 <- predict(mod6, newdata=dfTrainAll)

```


### Случайный лес (Random Forest, rf)

 RF.

```
 DataType        Size   Feat nTree Time  mtry   Acc
 CSMNR           37502, все, 100    17m   81      75.26%
 CSMNR+Lin       37502, все, 100      m   39      75.19%
 CSMNR+Lin       50000, все, 100      m   39      75.19%
 CSMNR+Lin+y0    50000, все, 100    17m   39      75.79%
 CSMNR+Lin+y06   50000, все, 100    m   39      %
```

```{r trainRandomForest, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE}

# dfTrainFull$yLin <- ifelse(dfTrainFull$yLin<0,0,dfTrainFull$yLin)
# dfTrainFull$yLin <- ifelse(dfTrainFull$yLin>6,6,dfTrainFull$yLin)
# 
# dfTestFull$yLin <- ifelse(dfTestFull$yLin<0,0,dfTestFull$yLin)
# dfTestFull$yLin <- ifelse(dfTestFull$yLin>6,6,dfTestFull$yLin)

dfTrainAll$yLin <- ifelse(dfTrainAll$yLin<0,0,dfTrainAll$yLin)
dfTrainAll$yLin <- ifelse(dfTrainAll$yLin>6,6,dfTrainAll$yLin)

#colnames(dfTrainAll)
nTree <- 499 #500
system.time({
    set.seed(1234)
    modRF <- train (y ~ ., method="rf", #rf, C5.0
                    #data = dfTrainFull [importantFeatures1], #
                    data = dfTrainAll, # финальный
                    #data = dfTrainFull, # 
                    trControl = trainControl(method = "cv", number=10, repeats=5),
                    #количество деревьев в лесу.
                    #странно, что при увеличении параметра точность иногда ухудшается, а иногда улучшается
                    ntree= nTree, 
                    tuneGrid = expand.grid(
                        ## RF
                        mtry=c(21, 35, 37, 39, 41, 43, 45)
                        ## C5.0
#                         trials=c(5,10,20),
#                         model="tree",
#                         winnow=FALSE
                        )
#                    tuneLength=3
    )
}) # 
modRF
ggplot(modRF) # пказывает, что достаточно 50 деревьев

modRF$finalModel
# layout(matrix(c(1,2),nrow=1),
#        width=c(4,1)) 
# par(mar=c(5,4,4,0)) #No margin on the right side
plot(modRF$finalModel, log="y")
#plot(modRF$finalModel)
# par(mar=c(5,0,4,2)) #No margin on the left side
# plot(c(0,1),type="n", axes=F, xlab="", ylab="")
# legend("top", colnames(modRF$finalModel$err.rate),col=1:8,cex=0.8,fill=1:8)


vi <- varImp(modRF)
vi <- vi$importance 
vi$varName <-  modRF$coefnames
viTop <- vi %>% arrange(desc(Overall)) %>% 
    top_n(50, Overall) 
viTop


#acc <- modRF$results$Accuracy[as.numeric(rownames(modRF$bestTune)[1])]
paramRF <- modRF$bestTune$mtry
```

За счет параметра y0 нам удалось снизить точность предсказания редкой возрастной группы 0 до ~0.7 (ранее была ~0.9).

?Попытка обучить случайный лес на сокращенном наборе наиболее значимых параметров дает менее точный результат.


Проведем анализ сдвига/разброса на основе Случайного леса c параметром mtry =`r paramRF`.
```{r biasAndVarianceRF, echo=FALSE, warning=FALSE, message = FALSE, cache=TRUE, eval=FALSE}
# на сокращенных данных
myPlotLearningCurve (dfTrain, dfTest, method = "rf", targetVariable = "y", 
                     featuresList = importantFeatures1,
                     learningCurvePoints = learningCurvePoints,
                     tuneGrid = data.frame(
                                            mtry=modRF$bestTune$mtry
                                            ),
                     title = "Random Forest",
                     cvRepeats = 3,
                     ntree = nTree)
#69 min на полной (37к)

dfResults  <- rbind(dfResults,
                    data.frame (model="Random Forest", 
                          accuracy = acc, variance="Очень высокий"))

```

При nTree=50 график показывает, что обучающая ошибка постоянно равна 0, а тестовая ~ 25%. Это означает, что модель переобучена (имеет высокий разброс) и сдвинута (имеет высокий сдвиг). При nTree = 100 график показывает то же самое.

Чтобы побороть проблему переобучения, можно применть следующее:

 - регуляризация (см. далее),
 - уменьшить количество деревьев,
 - попробовать на полной обучающей выборке,
 - метод bagging,
 - больше данных,
 - сокращение (отбор) параметров.

Поскольку данная модель показывает адекватные результаты, выделим параметры абонента, которые для нее оказались важными. Будем использовать этот набор параметров в случаях, когда другие модели будут показывать высокий разброс.





### ?Проверка лучшей модели на тестовой выборке 
 
?В качестве победившей по качеству предсказания на обучающих данных выбираем модель "Случайный лес (rf) ". Оценим качество ее предсказания на тестовой выборке, которую модель не "видела" при обучении.
 
```{r checkAccuracy, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
# делаем свою функцию, чтобы исправить баг библиотеки caret 6.0-41
myPredict.train <- function (object, newdata = NULL, type = "raw", na.action = na.omit, 
    ...) 
{
    if (all(names(object) != "modelInfo")) {
        object <- update(object, param = NULL)
    }
    if (!is.null(object$modelInfo$library)) 
        for (i in object$modelInfo$library) do.call("require", 
            list(package = i))
    if (!(type %in% c("raw", "prob"))) 
        stop("type must be either \"raw\" or \"prob\"")
    if (type == "prob") {
        if (is.null(object$modelInfo$prob)) 
            stop("only classification models that produce probabilities are allowed")
    }
    if (!is.null(newdata)) {
        if (inherits(object, "train.formula")) {
            newdata <- as.data.frame(newdata)
            rn <- row.names(newdata)
            Terms <- delete.response(object$terms)
            m <- model.frame(Terms, newdata, na.action = na.action, 
                xlev = object$xlevels)
            if (!is.null(cl <- attr(Terms, "dataClasses"))) 
                .checkMFClasses(cl, m)
            keep <- match(row.names(m), rn)
            newdata <- model.matrix(Terms, m, contrasts = object$contrasts)
            xint <- match("(Intercept)", colnames(newdata), nomatch = 0)
            if (xint > 0) 
                newdata <- newdata[, -xint, drop = FALSE]
        }
    }
    else {
        if (!is.null(object$trainingData)) {
            newdata <- if (object$method == "pam") 
                object$finalModel$xData
            else object$trainingData
        }
        else stop("please specify data via newdata")
    }
    if (type == "prob") {
        out <- extractProb(list(object), unkX = newdata, unkOnly = TRUE, 
            ...)
        obsLevels <- levels(object)
        out <- out[, paste0("X", obsLevels), drop = FALSE]
    }
    else {
        out <- extractPrediction(list(object), unkX = newdata, 
            unkOnly = TRUE, ...)$pred
    }
    out
}

#modFinal <- modGLMBoost #
#modFinal <- modSVM
#modFinal <- modLB #70,64% (полное обучение/тест)
#modFinal <- modDT # 
modFinal <- modRF # 75,92% (полное обучение/тестб CSMNR)
#modFinal <- modGBM #
#modFinal <- modRSNNS #
#modFinal <- modNNET #
#modFinal <- modGLMBoost # 71% на importantFeatures2, 75% на allFeatures
#debugonce(predict)
#predict(modFinal, newdata=dfTest, type="prob")

# Для всех, кроме modLB:
predTest <- predict(modFinal, newdata=dfTestFull) 
# # для modLB:
# predTest <- myPredict.train(modFinal, newdata=dfTestFull, type = "prob") 
# predTest <- (apply(predTest,1, which.max) - 1)
# predTest <- as.factor(predTest)

cm <- caret::confusionMatrix(data = predTest, reference = dfTestFull$y,
                             positive= "1")
accFinal <- cm$overal[1]
cm
```

## Предсказние новых 

```{r predictNew, echo=FALSE, warning=FALSE, message = FALSE, eval=TRUE}
modFinal <- modRF

#dfFinalTest <- readRDS("../data/test_ConstMedianRange.rds")
dfFinalTest <- readRDS("../data/test_ConstSubstMedianRange.rds")

# предсказываем дополнительный параметр
dfFinalTest$yLin <- predict (modGLMBoost, newdata = dfFinalTest)
dfFinalTest$y0 <- predict (mod0, newdata = dfFinalTest)
#dfFinalTest$y1 <- predict (mod1, newdata = dfFinalTest)
#dfFinalTest$y6 <- predict (mod6, newdata = dfFinalTest)

#table (dfTrainFull$y)
#apply(dfFinalTest, 2, function(x)sum(is.na(x)))

# для всех, кроме modLB (LogitBoost)
predicted <- predict (modFinal, newdata = dfFinalTest)
# # для LogitBoost
# # LogitBoost дает предсказание NA, если два класса получили одинаковые голоса.
# predicted <- myPredict.train (modLB, newdata = dfFinalTest, type = "prob") 
# predicted$y <- apply(predicted,1, which.max) - 1
# #View(predicted)
# predicted <- as.factor(predicted$y)

#03.10.15 - RF, сокр 30% = 74.91%
#05.10.15 - LogitBoost, полн 100% = 70.81%
#12.10.15 - RF на CSMNR, полн 100% = 75.73%
#12.10.15 - RF на CSMNR+yLin, полн 100% = 75.77%
#20.10.15 - RF на CSMNR+yLin, все данные тренировочные (50к) = 75.95%
#24.10.15 - RF на CSMNR+yLin+y0, все данные тренировочные (50к) = 76.07%
#24.10.15 - RF на CSMNR+yLin+y01, все данные тренировочные (50к) = 74.75% !!!
#24.10.15 - RF на CSMNR+yLin+y0, nTree=50, все данные тренировочные (50к) = 75.76% => nTree важен!
#24.10.15 - RF на CSMNR+yLin+y01, все данные тренировочные (50к) = 75.73% !!!
#24.10.15 - RF на CSMNR+yLin+y06, все данные тренировочные (50к) = 75.51% !!!

dfPredicted <- data.frame (ID = dfFinalTest$ID, y = predicted, stringsAsFactors = FALSE)
#View(dfPredicted)


write.table(dfPredicted, "../data/151028_sol_CostSubstMedianRange_yLin_y0_All.csv", sep=",", quote=FALSE, row.names=FALSE)
```


Результаты публикации:

 - для модели Случайный лес (rf) точность оказалась **74.91%**. Показатель невысокий, однако это первая публикация и она показала, что применяемый для выбора модели подход для оценки точности предстказания работает хорошо - в реальности точность оказалась даже чуть лучше, чем ожидалось.  
 - для модели (LogitBoost) точность оказалась **70.81%**. Низкая точность обусловлена спефикой модели (NA в предсказанных классах).  - для модели случайный лес на данных CSMNR (NA в категориях заменены на константу, категории в длинных переменных (> 5 категорий) заменены на наиболее вероятный для них класс, NA в новых категориях тестового примера также заменены на константу, числовые переменные преобразованы линейно в диапазон [0,1] ) точность оказалась **75.73%**.
- для модели случайный лес, аналогичной предыдущей, к которой был добавлен предсказанный непрерывный параметр возраста (yLin),  точность оказалась **75.77** .
- для модели случайный лес, аналогичной предыдущей, к которой был добавлен предсказанный бинарный параметр, показывающий принадлежность к возрастной группе 0 (y0),  точность оказалась **76.07%**.
 
 увеличить nTree

```{r stopCluster, echo=FALSE, warning=FALSE, message = FALSE, cache=FALSE}
stopCluster(cl) # Explicitly free up cores again.

```
